{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Web scrapping notebook for openrice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyOnxIXq8HE5"
      },
      "source": [
        "##最普通的方式抓取（获取全部餐厅的数据，只有250条）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLUQiTSOi8kd",
        "outputId": "7ff6402d-8fc1-42c7-ef10-b58177ec64d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=1 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 1 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=2 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 2 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=3 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 3 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=4 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 4 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=5 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 5 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=6 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 6 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=7 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 7 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=8 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 8 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=9 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 9 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=10 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 10 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=11 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 11 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=12 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 12 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=13 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 13 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=14 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 14 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=15 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 15 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=16 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 16 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=17 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 17 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=18 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 18 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=19 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 19 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=20 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 20 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=21 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 21 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=22 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 22 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=23 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 23 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=24 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 24 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=25 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 25 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=26 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 26 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=27 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 27 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=28 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 28 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=29 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 29 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=30 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 30 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=31 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 31 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=32 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 32 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=33 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 33 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=34 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 34 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=35 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 35 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=36 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 36 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=37 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 37 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n",
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=38 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 38 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=39 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 39 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=40 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 40 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=41 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 41 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=42 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 42 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=43 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 43 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=44 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 44 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=45 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 45 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=46 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 46 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=47 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 47 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=48 HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.openrice.com:443\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 48 页数据...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://www.openrice.com:443 \"GET /zh/hongkong/restaurants?page=49 HTTP/1.1\" 200 None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 49 页数据...\n",
            "数据获取并写入CSV文件完成。\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# 添加头部信息模拟浏览器请求\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "# 创建CSV文件并写入数据\n",
        "with open('restaurants.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['name', 'address', 'region', 'type1', 'type2', 'type3', 'person_price', 'well_rating', 'bad_rating'])  # 写入表头\n",
        "\n",
        "    # 循环遍历多个页面\n",
        "    for page_num in range(1, 50):\n",
        "        url = f'https://www.openrice.com/zh/hongkong/restaurants?page={page_num}'\n",
        "        response = requests.get(url, headers=headers)\n",
        "        print(f\"正在获取第 {page_num} 页数据...\")\n",
        "\n",
        "        # 解析网页内容\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # 定位并提取商户数据\n",
        "        restaurant_list = soup.find_all('div', class_='poi-list-cell-desktop-right')\n",
        "\n",
        "        for restaurant in restaurant_list:\n",
        "            name = restaurant.find('div', class_='text').text.strip()\n",
        "\n",
        "            # 获取地址信息\n",
        "            address_info = restaurant.find('div', class_='poi-list-cell-line-info').text.strip()\n",
        "            address_parts = address_info.split('/')\n",
        "            address = address_parts[0].split('\\n')[0].strip()  # 取第一个斜杠之前的部分，并删除额外的空格和换行符\n",
        "\n",
        "            # 获取食物类型信息\n",
        "            food_type_info = restaurant.find('div', class_='poi-list-cell-line-info-details').text.strip()\n",
        "            food_types = [t.strip() for t in food_type_info.split(' / ')]\n",
        "\n",
        "            # 补充缺失的食物类型，最多只取前3个\n",
        "            while len(food_types) < 3:\n",
        "                food_types.append('')\n",
        "\n",
        "            region = food_types.pop(0)  # 第一个是 region，剩下的是食物类型\n",
        "            type1 = food_types.pop(0)  # 取出第一个食物类型作为 type1\n",
        "\n",
        "            # 判断是否有 type2 和 type3\n",
        "            if len(food_types)>1:\n",
        "                # print(food_types)\n",
        "                type2 = food_types.pop(0)  # 取出第二个食物类型作为 type2\n",
        "                if '/' in type2:\n",
        "                    type2, type3 = type2.split('/', 1)\n",
        "                else:\n",
        "                    type3 = ''\n",
        "            else:\n",
        "                type2 = ''\n",
        "                type3 = ''\n",
        "\n",
        "\n",
        "\n",
        "            person_price = food_types.pop() if food_types else ''  # 最后一个是人均消费\n",
        "            well_rating = restaurant.find('div', class_='smile icon-wrapper big-score').text.strip()\n",
        "            bad_rating = restaurant.find('div', class_='cry icon-wrapper').text.strip()\n",
        "\n",
        "            # 将数据写入CSV文件\n",
        "            writer.writerow([name, address, region, type1, type2, type3, person_price, well_rating, bad_rating])\n",
        "\n",
        "print(\"数据获取并写入CSV文件完成。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ajnOR3I7jBF"
      },
      "source": [
        "##多线程获取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkWeT9Bszyc-",
        "outputId": "0c040f6e-6790-4529-dcbb-56d011537e4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在获取第 2 页数据...\n",
            "正在获取第 7 页数据...\n",
            "正在获取第 3 页数据...\n",
            "正在获取第 8 页数据...\n",
            "正在获取第 4 页数据...\n",
            "正在获取第 9 页数据...\n",
            "正在获取第 1 页数据...\n",
            "正在获取第 10 页数据...\n",
            "正在获取第 5 页数据...\n",
            "正在获取第 6 页数据...\n",
            "正在获取第 11 页数据...\n",
            "正在获取第 16 页数据...\n",
            "正在获取第 14 页数据...\n",
            "正在获取第 18 页数据...\n",
            "正在获取第 19 页数据...\n",
            "正在获取第 13 页数据...\n",
            "正在获取第 17 页数据...\n",
            "正在获取第 15 页数据...\n",
            "正在获取第 12 页数据...\n",
            "正在获取第 20 页数据...\n",
            "正在获取第 21 页数据...\n",
            "正在获取第 22 页数据...\n",
            "正在获取第 24 页数据...\n",
            "正在获取第 23 页数据...\n",
            "正在获取第 25 页数据...\n",
            "正在获取第 26 页数据...\n",
            "正在获取第 28 页数据...\n",
            "正在获取第 27 页数据...\n",
            "正在获取第 29 页数据...\n",
            "正在获取第 30 页数据...\n",
            "正在获取第 31 页数据...\n",
            "正在获取第 33 页数据...\n",
            "正在获取第 32 页数据...\n",
            "正在获取第 34 页数据...\n",
            "正在获取第 35 页数据...\n",
            "正在获取第 37 页数据...\n",
            "正在获取第 36 页数据...\n",
            "正在获取第 38 页数据...\n",
            "正在获取第 39 页数据...\n",
            "正在获取第 40 页数据...\n",
            "正在获取第 41 页数据...\n",
            "正在获取第 42 页数据...\n",
            "正在获取第 44 页数据...\n",
            "正在获取第 43 页数据...\n",
            "正在获取第 45 页数据...\n",
            "正在获取第 46 页数据...\n",
            "正在获取第 47 页数据...\n",
            "正在获取第 48 页数据...\n",
            "正在获取第 49 页数据...\n",
            "正在获取第 50 页数据...\n",
            "正在获取第 51 页数据...\n",
            "正在获取第 52 页数据...\n",
            "正在获取第 53 页数据...\n",
            "正在获取第 54 页数据...\n",
            "正在获取第 56 页数据...\n",
            "正在获取第 57 页数据...\n",
            "正在获取第 55 页数据...\n",
            "正在获取第 58 页数据...\n",
            "正在获取第 59 页数据...\n",
            "正在获取第 60 页数据...\n",
            "正在获取第 62 页数据...\n",
            "正在获取第 61 页数据...\n",
            "正在获取第 63 页数据...\n",
            "正在获取第 67 页数据...\n",
            "正在获取第 66 页数据...\n",
            "正在获取第 65 页数据...\n",
            "正在获取第 64 页数据...\n",
            "正在获取第 68 页数据...\n",
            "正在获取第 69 页数据...\n",
            "正在获取第 70 页数据...\n",
            "正在获取第 72 页数据...\n",
            "正在获取第 71 页数据...\n",
            "正在获取第 73 页数据...\n",
            "正在获取第 74 页数据...\n",
            "正在获取第 76 页数据...\n",
            "正在获取第 75 页数据...\n",
            "正在获取第 77 页数据...\n",
            "正在获取第 78 页数据...\n",
            "正在获取第 79 页数据...\n",
            "正在获取第 80 页数据...\n",
            "正在获取第 86 页数据...\n",
            "正在获取第 84 页数据...\n",
            "正在获取第 87 页数据...\n",
            "正在获取第 83 页数据...\n",
            "正在获取第 89 页数据...\n",
            "正在获取第 85 页数据...\n",
            "正在获取第 88 页数据...\n",
            "正在获取第 90 页数据...\n",
            "正在获取第 81 页数据...\n",
            "正在获取第 82 页数据...\n",
            "正在获取第 91 页数据...\n",
            "正在获取第 97 页数据...\n",
            "正在获取第 96 页数据...\n",
            "正在获取第 93 页数据...\n",
            "正在获取第 94 页数据...\n",
            "正在获取第 95 页数据...\n",
            "正在获取第 92 页数据...\n",
            "正在获取第 99 页数据...\n",
            "正在获取第 100 页数据...\n",
            "正在获取第 98 页数据...\n",
            "正在获取第 101 页数据...\n",
            "正在获取第 102 页数据...\n",
            "正在获取第 104 页数据...\n",
            "正在获取第 103 页数据...\n",
            "正在获取第 109 页数据...\n",
            "正在获取第 106 页数据...\n",
            "正在获取第 105 页数据...\n",
            "正在获取第 108 页数据...\n",
            "正在获取第 107 页数据...\n",
            "正在获取第 110 页数据...\n",
            "正在获取第 111 页数据...\n",
            "正在获取第 113 页数据...\n",
            "正在获取第 112 页数据...\n",
            "正在获取第 114 页数据...\n",
            "正在获取第 115 页数据...\n",
            "正在获取第 116 页数据...\n",
            "正在获取第 117 页数据...\n",
            "正在获取第 118 页数据...\n",
            "正在获取第 119 页数据...\n",
            "正在获取第 120 页数据...\n",
            "正在获取第 121 页数据...\n",
            "正在获取第 125 页数据...\n",
            "正在获取第 123 页数据...\n",
            "正在获取第 122 页数据...\n",
            "正在获取第 124 页数据...\n",
            "正在获取第 128 页数据...\n",
            "正在获取第 127 页数据...\n",
            "正在获取第 126 页数据...\n",
            "正在获取第 130 页数据...\n",
            "正在获取第 129 页数据...\n",
            "正在获取第 131 页数据...\n",
            "正在获取第 132 页数据...\n",
            "正在获取第 133 页数据...\n",
            "正在获取第 136 页数据...\n",
            "正在获取第 140 页数据...\n",
            "正在获取第 139 页数据...\n",
            "正在获取第 134 页数据...\n",
            "正在获取第 137 页数据...\n",
            "正在获取第 135 页数据...\n",
            "正在获取第 138 页数据...\n",
            "正在获取第 141 页数据...\n",
            "正在获取第 145 页数据...\n",
            "正在获取第 142 页数据...\n",
            "正在获取第 146 页数据...\n",
            "正在获取第 144 页数据...\n",
            "正在获取第 148 页数据...\n",
            "正在获取第 149 页数据...\n",
            "正在获取第 143 页数据...\n",
            "正在获取第 147 页数据...\n",
            "正在获取第 150 页数据...\n",
            "正在获取第 151 页数据...\n",
            "正在获取第 152 页数据...\n",
            "正在获取第 153 页数据...\n",
            "正在获取第 154 页数据...\n",
            "正在获取第 156 页数据...\n",
            "正在获取第 158 页数据...\n",
            "正在获取第 155 页数据...\n",
            "正在获取第 159 页数据...\n",
            "正在获取第 157 页数据...\n",
            "正在获取第 160 页数据...\n",
            "正在获取第 161 页数据...\n",
            "正在获取第 163 页数据...\n",
            "正在获取第 162 页数据...\n",
            "正在获取第 164 页数据...\n",
            "正在获取第 165 页数据...\n",
            "正在获取第 167 页数据...\n",
            "正在获取第 166 页数据...\n",
            "正在获取第 168 页数据...\n",
            "正在获取第 169 页数据...\n",
            "正在获取第 170 页数据...\n",
            "正在获取第 171 页数据...\n",
            "正在获取第 174 页数据...\n",
            "正在获取第 173 页数据...\n",
            "正在获取第 176 页数据...\n",
            "正在获取第 175 页数据...\n",
            "正在获取第 172 页数据...\n",
            "正在获取第 178 页数据...\n",
            "正在获取第 177 页数据...\n",
            "正在获取第 180 页数据...\n",
            "正在获取第 179 页数据...\n",
            "正在获取第 181 页数据...\n",
            "正在获取第 182 页数据...\n",
            "正在获取第 184 页数据...\n",
            "正在获取第 183 页数据...\n",
            "正在获取第 187 页数据...\n",
            "正在获取第 185 页数据...\n",
            "正在获取第 186 页数据...\n",
            "正在获取第 188 页数据...\n",
            "正在获取第 190 页数据...\n",
            "正在获取第 189 页数据...\n",
            "正在获取第 191 页数据...\n",
            "正在获取第 192 页数据...\n",
            "正在获取第 193 页数据...\n",
            "正在获取第 197 页数据...\n",
            "正在获取第 195 页数据...\n",
            "正在获取第 196 页数据...\n",
            "正在获取第 198 页数据...\n",
            "正在获取第 194 页数据...\n",
            "正在获取第 200 页数据...\n",
            "正在获取第 199 页数据...\n",
            "正在获取第 201 页数据...\n",
            "正在获取第 202 页数据...\n",
            "正在获取第 203 页数据...\n",
            "正在获取第 205 页数据...\n",
            "正在获取第 206 页数据...\n",
            "正在获取第 204 页数据...\n",
            "正在获取第 208 页数据...\n",
            "正在获取第 207 页数据...\n",
            "正在获取第 210 页数据...\n",
            "正在获取第 209 页数据...\n",
            "正在获取第 211 页数据...\n",
            "正在获取第 212 页数据...\n",
            "正在获取第 213 页数据...\n",
            "正在获取第 216 页数据...\n",
            "正在获取第 214 页数据...\n",
            "正在获取第 215 页数据...\n",
            "正在获取第 217 页数据...\n",
            "正在获取第 218 页数据...\n",
            "正在获取第 219 页数据...\n",
            "正在获取第 220 页数据...\n",
            "正在获取第 221 页数据...\n",
            "正在获取第 222 页数据...\n",
            "正在获取第 223 页数据...\n",
            "正在获取第 224 页数据...\n",
            "正在获取第 225 页数据...\n",
            "正在获取第 227 页数据...\n",
            "正在获取第 226 页数据...\n",
            "正在获取第 228 页数据...\n",
            "正在获取第 230 页数据...\n",
            "正在获取第 229 页数据...\n",
            "正在获取第 231 页数据...\n",
            "正在获取第 235 页数据...\n",
            "正在获取第 232 页数据...\n",
            "正在获取第 233 页数据...\n",
            "正在获取第 234 页数据...\n",
            "正在获取第 236 页数据...\n",
            "正在获取第 239 页数据...\n",
            "正在获取第 238 页数据...\n",
            "正在获取第 240 页数据...\n",
            "正在获取第 237 页数据...\n",
            "正在获取第 241 页数据...\n",
            "正在获取第 242 页数据...\n",
            "正在获取第 245 页数据...\n",
            "正在获取第 243 页数据...\n",
            "正在获取第 244 页数据...\n",
            "正在获取第 246 页数据...\n",
            "正在获取第 247 页数据...\n",
            "正在获取第 250 页数据...\n",
            "正在获取第 249 页数据...\n",
            "正在获取第 248 页数据...\n",
            "正在获取第 251 页数据...\n",
            "正在获取第 252 页数据...\n",
            "正在获取第 253 页数据...\n",
            "正在获取第 255 页数据...\n",
            "正在获取第 254 页数据...\n",
            "正在获取第 256 页数据...\n",
            "正在获取第 257 页数据...\n",
            "正在获取第 258 页数据...\n",
            "正在获取第 259 页数据...\n",
            "正在获取第 260 页数据...\n",
            "正在获取第 262 页数据...\n",
            "正在获取第 261 页数据...\n",
            "正在获取第 263 页数据...\n",
            "正在获取第 264 页数据...\n",
            "正在获取第 266 页数据...\n",
            "正在获取第 265 页数据...\n",
            "正在获取第 267 页数据...\n",
            "正在获取第 269 页数据...\n",
            "正在获取第 270 页数据...\n",
            "正在获取第 268 页数据...\n",
            "正在获取第 272 页数据...\n",
            "正在获取第 271 页数据...\n",
            "正在获取第 275 页数据...\n",
            "正在获取第 273 页数据...\n",
            "正在获取第 276 页数据...\n",
            "正在获取第 274 页数据...\n",
            "正在获取第 277 页数据...\n",
            "正在获取第 278 页数据...\n",
            "正在获取第 280 页数据...\n",
            "正在获取第 279 页数据...\n",
            "正在获取第 282 页数据...\n",
            "正在获取第 283 页数据...\n",
            "正在获取第 286 页数据...\n",
            "正在获取第 281 页数据...\n",
            "正在获取第 288 页数据...\n",
            "正在获取第 285 页数据...\n",
            "正在获取第 284 页数据...\n",
            "正在获取第 289 页数据...\n",
            "正在获取第 287 页数据...\n",
            "正在获取第 290 页数据...\n",
            "正在获取第 292 页数据...\n",
            "正在获取第 293 页数据...\n",
            "正在获取第 291 页数据...\n",
            "正在获取第 294 页数据...\n",
            "正在获取第 298 页数据...\n",
            "正在获取第 299 页数据...\n",
            "正在获取第 296 页数据...\n",
            "正在获取第 295 页数据...\n",
            "正在获取第 300 页数据...\n",
            "正在获取第 297 页数据...\n",
            "正在获取第 301 页数据...\n",
            "正在获取第 302 页数据...\n",
            "正在获取第 303 页数据...\n",
            "正在获取第 306 页数据...\n",
            "正在获取第 304 页数据...\n",
            "正在获取第 305 页数据...\n",
            "正在获取第 308 页数据...\n",
            "正在获取第 307 页数据...\n",
            "正在获取第 309 页数据...\n",
            "正在获取第 312 页数据...\n",
            "正在获取第 311 页数据...\n",
            "正在获取第 310 页数据...\n",
            "正在获取第 315 页数据...\n",
            "正在获取第 316 页数据...\n",
            "正在获取第 317 页数据...\n",
            "正在获取第 314 页数据...\n",
            "正在获取第 318 页数据...\n",
            "正在获取第 313 页数据...\n",
            "正在获取第 320 页数据...正在获取第 319 页数据...\n",
            "\n",
            "正在获取第 322 页数据...\n",
            "正在获取第 321 页数据...\n",
            "正在获取第 324 页数据...\n",
            "正在获取第 323 页数据...\n",
            "正在获取第 328 页数据...\n",
            "正在获取第 326 页数据...\n",
            "正在获取第 325 页数据...\n",
            "正在获取第 327 页数据...\n",
            "正在获取第 330 页数据...\n",
            "正在获取第 331 页数据...\n",
            "正在获取第 329 页数据...\n",
            "正在获取第 332 页数据...\n",
            "正在获取第 333 页数据...\n",
            "正在获取第 334 页数据...\n",
            "正在获取第 335 页数据...\n",
            "正在获取第 338 页数据...\n",
            "正在获取第 336 页数据...\n",
            "正在获取第 337 页数据...\n",
            "正在获取第 341 页数据...\n",
            "正在获取第 339 页数据...\n",
            "正在获取第 340 页数据...\n",
            "正在获取第 342 页数据...\n",
            "正在获取第 343 页数据...\n",
            "正在获取第 344 页数据...\n",
            "正在获取第 348 页数据...\n",
            "正在获取第 345 页数据...\n",
            "正在获取第 347 页数据...\n",
            "正在获取第 346 页数据...\n",
            "正在获取第 350 页数据...\n",
            "正在获取第 349 页数据...\n",
            "正在获取第 351 页数据...\n",
            "正在获取第 353 页数据...\n",
            "正在获取第 355 页数据...\n",
            "正在获取第 352 页数据...\n",
            "正在获取第 354 页数据...\n",
            "正在获取第 357 页数据...\n",
            "正在获取第 358 页数据...\n",
            "正在获取第 356 页数据...\n",
            "正在获取第 359 页数据...\n",
            "正在获取第 360 页数据...\n",
            "正在获取第 361 页数据...\n",
            "正在获取第 362 页数据...\n",
            "正在获取第 363 页数据...\n",
            "正在获取第 364 页数据...\n",
            "正在获取第 365 页数据...\n",
            "正在获取第 368 页数据...\n",
            "正在获取第 366 页数据...\n",
            "正在获取第 367 页数据...\n",
            "正在获取第 369 页数据...\n",
            "正在获取第 370 页数据...\n",
            "正在获取第 371 页数据...\n",
            "正在获取第 374 页数据...\n",
            "正在获取第 372 页数据...\n",
            "正在获取第 375 页数据...\n",
            "正在获取第 377 页数据...\n",
            "正在获取第 379 页数据...\n",
            "正在获取第 376 页数据...\n",
            "正在获取第 373 页数据...\n",
            "正在获取第 380 页数据...\n",
            "正在获取第 381 页数据...\n",
            "正在获取第 378 页数据...\n",
            "正在获取第 384 页数据...\n",
            "正在获取第 385 页数据...\n",
            "正在获取第 382 页数据...\n",
            "正在获取第 383 页数据...\n",
            "正在获取第 386 页数据...\n",
            "正在获取第 387 页数据...\n",
            "正在获取第 389 页数据...\n",
            "正在获取第 390 页数据...\n",
            "正在获取第 391 页数据...\n",
            "正在获取第 388 页数据...\n",
            "正在获取第 393 页数据...\n",
            "正在获取第 392 页数据...\n",
            "正在获取第 394 页数据...\n",
            "正在获取第 395 页数据...\n",
            "正在获取第 397 页数据...\n",
            "正在获取第 396 页数据...\n",
            "正在获取第 400 页数据...\n",
            "正在获取第 398 页数据...\n",
            "正在获取第 399 页数据...\n",
            "正在获取第 403 页数据...\n",
            "正在获取第 405 页数据...\n",
            "正在获取第 401 页数据...\n",
            "正在获取第 404 页数据...\n",
            "正在获取第 402 页数据...\n",
            "正在获取第 407 页数据...\n",
            "正在获取第 406 页数据...\n",
            "正在获取第 410 页数据...\n",
            "正在获取第 412 页数据...\n",
            "正在获取第 408 页数据...\n",
            "正在获取第 411 页数据...\n",
            "正在获取第 409 页数据...\n",
            "正在获取第 414 页数据...\n",
            "正在获取第 413 页数据...\n",
            "正在获取第 415 页数据...\n",
            "正在获取第 416 页数据...\n",
            "正在获取第 417 页数据...\n",
            "正在获取第 419 页数据...\n",
            "正在获取第 421 页数据...\n",
            "正在获取第 420 页数据...\n",
            "正在获取第 418 页数据...\n",
            "正在获取第 422 页数据...\n",
            "正在获取第 424 页数据...\n",
            "正在获取第 423 页数据...\n",
            "正在获取第 425 页数据...\n",
            "正在获取第 426 页数据...\n",
            "正在获取第 427 页数据...\n",
            "正在获取第 428 页数据...\n",
            "正在获取第 429 页数据...\n",
            "正在获取第 430 页数据...\n",
            "正在获取第 432 页数据...\n",
            "正在获取第 434 页数据...\n",
            "正在获取第 433 页数据...\n",
            "正在获取第 431 页数据...\n",
            "正在获取第 436 页数据...\n",
            "正在获取第 435 页数据...\n",
            "正在获取第 437 页数据...\n",
            "正在获取第 438 页数据...\n",
            "正在获取第 439 页数据...\n",
            "正在获取第 443 页数据...\n",
            "正在获取第 442 页数据...\n",
            "正在获取第 444 页数据...\n",
            "正在获取第 441 页数据...\n",
            "正在获取第 440 页数据...\n",
            "正在获取第 445 页数据...\n",
            "正在获取第 446 页数据...\n",
            "正在获取第 448 页数据...\n",
            "正在获取第 451 页数据...\n",
            "正在获取第 447 页数据...\n",
            "正在获取第 453 页数据...\n",
            "正在获取第 452 页数据...\n",
            "正在获取第 454 页数据...\n",
            "正在获取第 450 页数据...\n",
            "正在获取第 449 页数据...\n",
            "正在获取第 455 页数据...\n",
            "正在获取第 456 页数据...\n",
            "正在获取第 457 页数据...\n",
            "正在获取第 459 页数据...\n",
            "正在获取第 458 页数据...\n",
            "正在获取第 460 页数据...\n",
            "正在获取第 464 页数据...\n",
            "正在获取第 461 页数据...\n",
            "正在获取第 465 页数据...\n",
            "正在获取第 462 页数据...\n",
            "正在获取第 463 页数据...\n",
            "正在获取第 466 页数据...\n",
            "正在获取第 467 页数据...\n",
            "正在获取第 468 页数据...\n",
            "正在获取第 469 页数据...\n",
            "正在获取第 471 页数据...\n",
            "正在获取第 472 页数据...\n",
            "正在获取第 475 页数据...\n",
            "正在获取第 474 页数据...\n",
            "正在获取第 470 页数据...\n",
            "正在获取第 473 页数据...\n",
            "正在获取第 476 页数据...\n",
            "正在获取第 478 页数据...\n",
            "正在获取第 477 页数据...\n",
            "正在获取第 479 页数据...\n",
            "正在获取第 480 页数据...\n",
            "正在获取第 481 页数据...\n",
            "正在获取第 485 页数据...\n",
            "正在获取第 484 页数据...\n",
            "正在获取第 483 页数据...\n",
            "正在获取第 486 页数据...\n",
            "正在获取第 482 页数据...\n",
            "正在获取第 487 页数据...\n",
            "正在获取第 488 页数据...\n",
            "正在获取第 489 页数据...\n",
            "正在获取第 490 页数据...\n",
            "正在获取第 492 页数据...\n",
            "正在获取第 491 页数据...\n",
            "正在获取第 494 页数据...\n",
            "正在获取第 493 页数据...\n",
            "正在获取第 495 页数据...\n",
            "正在获取第 497 页数据...\n",
            "正在获取第 498 页数据...\n",
            "正在获取第 496 页数据...\n",
            "正在获取第 499 页数据...\n",
            "数据获取并写入CSV文件完成。\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import concurrent.futures\n",
        "\n",
        "# 添加头部信息模拟浏览器请求\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "def fetch_and_parse_data(page_num):\n",
        "    url = f'https://www.openrice.com/zh/hongkong/restaurants?page={page_num}'\n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(f\"正在获取第 {page_num} 页数据...\")\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    restaurant_list = soup.find_all('div', class_='poi-list-cell-desktop-right')\n",
        "\n",
        "    data = []\n",
        "    for restaurant in restaurant_list:\n",
        "        name = restaurant.find('div', class_='text').text.strip()\n",
        "\n",
        "        # 获取地址信息\n",
        "        address_info = restaurant.find('div', class_='poi-list-cell-line-info').text.strip()\n",
        "        address_parts = address_info.split('/')\n",
        "        address = address_parts[0].split('\\n')[0].strip()\n",
        "\n",
        "        # 获取食物类型信息\n",
        "        food_type_info = restaurant.find('div', class_='poi-list-cell-line-info-details').text.strip()\n",
        "        food_types = [t.strip() for t in food_type_info.split(' / ')]\n",
        "\n",
        "        while len(food_types) < 3:\n",
        "            food_types.append('')\n",
        "\n",
        "        region = food_types.pop(0)\n",
        "        type1 = food_types.pop(0)\n",
        "\n",
        "        if len(food_types) > 1:\n",
        "            type2 = food_types.pop(0)\n",
        "            if '/' in type2:\n",
        "                type2, type3 = type2.split('/', 1)\n",
        "            else:\n",
        "                type3 = ''\n",
        "        else:\n",
        "            type2 = ''\n",
        "            type3 = ''\n",
        "\n",
        "        person_price = food_types.pop() if food_types else ''\n",
        "        well_rating = restaurant.find('div', class_='smile icon-wrapper big-score').text.strip()\n",
        "        bad_rating = restaurant.find('div', class_='cry icon-wrapper').text.strip()\n",
        "\n",
        "        data.append([name, address, region, type1, type2, type3, person_price, well_rating, bad_rating])\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_to_csv(data):\n",
        "    with open('restaurants.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "    with open('restaurants.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['name', 'address', 'region', 'type1', 'type2', 'type3', 'person_price', 'well_rating', 'bad_rating'])  # 写入表头\n",
        "\n",
        "    pages = range(1, 500)  # 假设要获取前50页的数据\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        results = executor.map(fetch_and_parse_data, pages)\n",
        "\n",
        "        for result in results:\n",
        "            write_to_csv(result)\n",
        "\n",
        "    print(\"数据获取并写入CSV文件完成。\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''\n",
        "Use different url headings and approach to get the data\n",
        "'''\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
        "}\n",
        "\n",
        "# https://www.openrice.com/en/hongkong/restaurants?page={}&searchSort=31&region=0&district_id={} \n",
        "\n",
        "#https://www.openrice.com/en/hongkong/restaurants?cuisineId=3006&priceRangeId=3&tabIndex=0&tabType=\n",
        "# https://www.openrice.com/en/hongkong/restaurants?sortBy=ORScoreDesc&cuisineId=3006&priceRangeId=3&tabIndex=0&tabType=\n",
        "\n",
        "#https://www.openrice.com/en/hongkong/restaurants/cuisine/italian?page=1&searchSort=31&region=0&priceRangeId=3&tabIndex=0&tabType=\n",
        "\n",
        "# use different url headings\n",
        "\n",
        "def fetch_and_parse_data(page_num):\n",
        "    url_1 = 'https://www.openrice.com/zh/hongkong/restaurants?page='\n",
        "    url_2 = '&searchSort=31&region=0&district_id='\n",
        "    \n",
        "    response = requests.get(url, headers=headers)\n",
        "    print(f\"正在获取第 {page_num} 页数据...\")\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    restaurant_list = soup.find_all('div', class_='poi-list-cell-desktop-right')\n",
        "\n",
        "    data = []\n",
        "    for restaurant in restaurant_list:\n",
        "        name = restaurant.find('div', class_='text').text.strip()\n",
        "\n",
        "        # 获取地址信息\n",
        "        address_info = restaurant.find('div', class_='poi-list-cell-line-info').text.strip()\n",
        "        address_parts = address_info.split('/')\n",
        "        address = address_parts[0].split('\\n')[0].strip()\n",
        "\n",
        "        # 获取食物类型信息\n",
        "        food_type_info = restaurant.find('div', class_='poi-list-cell-line-info-details').text.strip()\n",
        "        food_types = [t.strip() for t in food_type_info.split(' / ')]\n",
        "\n",
        "        while len(food_types) < 3:\n",
        "            food_types.append('')\n",
        "\n",
        "        region = food_types.pop(0)\n",
        "        type1 = food_types.pop(0)\n",
        "\n",
        "        if len(food_types) > 1:\n",
        "            type2 = food_types.pop(0)\n",
        "            if '/' in type2:\n",
        "                type2, type3 = type2.split('/', 1)\n",
        "            else:\n",
        "                type3 = ''\n",
        "        else:\n",
        "            type2 = ''\n",
        "            type3 = ''\n",
        "\n",
        "        person_price = food_types.pop() if food_types else ''\n",
        "        well_rating = restaurant.find('div', class_='smile icon-wrapper big-score').text.strip()\n",
        "        bad_rating = restaurant.find('div', class_='cry icon-wrapper').text.strip()\n",
        "\n",
        "        data.append([name, address, region, type1, type2, type3, person_price, well_rating, bad_rating])\n",
        "\n",
        "    return data\n",
        "\n",
        "def write_to_csv(data):\n",
        "    with open('restaurants.csv', 'a', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerows(data)\n",
        "\n",
        "def main():\n",
        "    with open('restaurants.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['name', 'address', 'region', 'type1', 'type2', 'type3', 'person_price', 'well_rating', 'bad_rating'])  # 写入表头\n",
        "\n",
        "    pages = range(1, 500)  # 假设要获取前50页的数据\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        results = executor.map(fetch_and_parse_data, pages)\n",
        "\n",
        "        for result in results:\n",
        "            write_to_csv(result)\n",
        "\n",
        "    print(\"数据获取并写入CSV文件完成。\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGyTm4nr8BFk"
      },
      "source": [
        "##使用selenium抓取（动态抓取不成功）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7U80hCC5mZk",
        "outputId": "0b2e2b9e-b7d0-42be-83cf-3b783b2a2899"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "  Downloading Scrapy-2.11.1-py2.py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.8/287.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Twisted>=18.9.0 (from scrapy)\n",
            "  Downloading twisted-24.3.0-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (42.0.5)\n",
            "Collecting cssselect>=0.9.1 (from scrapy)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy)\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy)\n",
            "  Downloading parsel-1.9.0-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.1.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy)\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy)\n",
            "  Downloading service_identity-24.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy)\n",
            "  Downloading zope.interface-6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy)\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (24.0)\n",
            "Collecting tldextract (from scrapy)\n",
            "  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.4)\n",
            "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Collecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting incremental>=22.10.0 (from Twisted>=18.9.0->scrapy)\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=18.9.0->scrapy) (4.10.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.6)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from automat>=0.8.0->Twisted>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\n",
            "Installing collected packages: PyDispatcher, incremental, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\n",
            "Successfully installed PyDispatcher-2.0.7 Twisted-24.3.0 automat-22.10.0 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 parsel-1.9.0 protego-0.3.0 queuelib-1.6.2 requests-file-2.0.0 scrapy-2.11.1 service-identity-24.1.0 tldextract-5.1.2 w3lib-2.1.2 zope.interface-6.2\n"
          ]
        }
      ],
      "source": [
        "!pip install scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAWDsxiBKr74"
      },
      "outputs": [],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEpMkcK6Oaij",
        "outputId": "b811960c-a066-4fcb-e795-17b1cb6c95c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "chromium-chromedriver is already the newest version (1:85.0.4183.83-0ubuntu2.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt install chromium-chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukQUIn9_O9Uo",
        "outputId": "d4778ffb-ecce-4a32-820e-5c9cf274f510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/bin/chromedriver\n"
          ]
        }
      ],
      "source": [
        "!which chromedriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOaQsTR-S2sB",
        "outputId": "a063b131-d436-4545-ab57-880bde1eb814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "158.247.226.12\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "# 获取当前的 IP 地址\n",
        "print(requests.get('https://api.ipify.org').text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gga97Ay4UHnr"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'selenium'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Options\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Chrome options\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Chrome options\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "\n",
        "# Initialize Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Website URL to open\n",
        "website_url = 'https://www.openrice.com/zh/hongkong/restaurants/'  # Change this to the website you want to test\n",
        "\n",
        "try:\n",
        "    # Open the website\n",
        "    driver.get(website_url)\n",
        "    print('链接成功')\n",
        "    # Print the page title to verify if the website is opened successfully\n",
        "    print(\"Page Title:\", driver.title)\n",
        "\n",
        "finally:\n",
        "    # Quit the WebDriver\n",
        "    driver.quit()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzQULBQB8E_3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-tD6uz8W-3f"
      },
      "source": [
        "##导出为静态页面进行获取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T01kzQlKnej9",
        "outputId": "8efddc9d-f09b-44a1-e26e-b6a8a614202d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据获取并写入CSV文件完成。\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# 创建CSV文件并写入数据\n",
        "with open('restaurants_all.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['name', 'address', 'region', 'type1', 'type2', 'type3', 'person_price', 'well_rating', 'bad_rating'])  # 写入表头\n",
        "\n",
        "    # 打开静态页面文件\n",
        "    with open('22.html', 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "        # 解析静态页面内容\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # 定位并提取商户数据\n",
        "        restaurant_list = soup.find_all('div', class_='poi-list-cell-desktop-right')\n",
        "\n",
        "        for restaurant in restaurant_list:\n",
        "            name = restaurant.find('div', class_='text').text.strip()\n",
        "\n",
        "            # 获取地址信息\n",
        "            address_info = restaurant.find('div', class_='poi-list-cell-line-info').text.strip()\n",
        "            address_parts = address_info.split('/')\n",
        "            address = address_parts[0].split('\\n')[0].strip()  # 取第一个斜杠之前的部分，并删除额外的空格和换行符\n",
        "\n",
        "            # 获取食物类型信息\n",
        "            food_type_info = restaurant.find('div', class_='poi-list-cell-line-info-details').text.strip()\n",
        "            food_types = [t.strip() for t in food_type_info.split(' / ')]\n",
        "\n",
        "            # 补充缺失的食物类型，最多只取前3个\n",
        "            while len(food_types) < 3:\n",
        "                food_types.append('')\n",
        "\n",
        "            region = food_types.pop(0)  # 第一个是 region，剩下的是食物类型\n",
        "            type1 = food_types.pop(0)  # 取出第一个食物类型作为 type1\n",
        "\n",
        "            # 判断是否有 type2 和 type3\n",
        "            if len(food_types) > 1:\n",
        "                type2 = food_types.pop(0)  # 取出第二个食物类型作为 type2\n",
        "                if '/' in type2:\n",
        "                    type2, type3 = type2.split('/', 1)\n",
        "                else:\n",
        "                    type3 = ''\n",
        "            else:\n",
        "                type2 = ''\n",
        "                type3 = ''\n",
        "\n",
        "            person_price = food_types.pop() if food_types else ''  # 最后一个是人均消费\n",
        "            well_rating = restaurant.find('div', class_='smile icon-wrapper big-score').text.strip()\n",
        "            bad_rating = restaurant.find('div', class_='cry icon-wrapper').text.strip()\n",
        "\n",
        "            # 将数据写入CSV文件\n",
        "            writer.writerow([name, address, region, type1, type2, type3, person_price, well_rating, bad_rating])\n",
        "\n",
        "print(\"数据获取并写入CSV文件完成。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOObd2RAU4eG",
        "outputId": "b21df0d7-0d38-4c91-cd66-3a57176527ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据获取并写入CSV文件完成。\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# 创建CSV文件并写入数据\n",
        "with open('restaurants_Italian.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['name', 'address', 'region', 'type1', 'type2', 'type3', 'person_price', 'well_rating', 'bad_rating'])  # 写入表头\n",
        "\n",
        "    # 打开静态页面文件\n",
        "    with open('11.html', 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "        # 解析静态页面内容\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # 定位并提取商户数据\n",
        "        restaurant_list = soup.find_all('div', class_='poi-list-cell-desktop-right')\n",
        "\n",
        "        for restaurant in restaurant_list:\n",
        "            name = restaurant.find('div', class_='text').text.strip()\n",
        "\n",
        "            # 获取地址信息\n",
        "            address_info = restaurant.find('div', class_='poi-list-cell-line-info').text.strip()\n",
        "            address_parts = address_info.split('/')\n",
        "            address = address_parts[0].split('\\n')[0].strip()  # 取第一个斜杠之前的部分，并删除额外的空格和换行符\n",
        "\n",
        "            # 获取食物类型信息\n",
        "            food_type_info = restaurant.find('div', class_='poi-list-cell-line-info-details').text.strip()\n",
        "            food_types = [t.strip() for t in food_type_info.split(' / ')]\n",
        "\n",
        "            # 补充缺失的食物类型，最多只取前3个\n",
        "            while len(food_types) < 3:\n",
        "                food_types.append('')\n",
        "\n",
        "            region = food_types.pop(0)  # 第一个是 region，剩下的是食物类型\n",
        "            type1 = food_types.pop(0)  # 取出第一个食物类型作为 type1\n",
        "\n",
        "            # 判断是否有 type2 和 type3\n",
        "            if len(food_types) > 1:\n",
        "                type2 = food_types.pop(0)  # 取出第二个食物类型作为 type2\n",
        "                if '/' in type2:\n",
        "                    type2, type3 = type2.split('/', 1)\n",
        "                else:\n",
        "                    type3 = ''\n",
        "            else:\n",
        "                type2 = ''\n",
        "                type3 = ''\n",
        "\n",
        "            person_price = food_types.pop() if food_types else ''  # 最后一个是人均消费\n",
        "            well_rating = restaurant.find('div', class_='smile icon-wrapper big-score').text.strip()\n",
        "            bad_rating = restaurant.find('div', class_='cry icon-wrapper').text.strip()\n",
        "\n",
        "            # 将数据写入CSV文件\n",
        "            writer.writerow([name, address, region, type1, type2, type3, person_price, well_rating, bad_rating])\n",
        "\n",
        "print(\"数据获取并写入CSV文件完成。\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzC0wtAa38vT",
        "outputId": "4b28b80b-12de-4f5c-c74e-7c50d8ac7b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "数据获取并写入CSV文件完成。\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# 创建CSV文件并写入数据\n",
        "with open('restaurants_Italian100.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "    writer.writerow(['name', 'address', 'region', 'type1', 'type2', 'type3', 'person_price', 'well_rating', 'bad_rating'])  # 写入表头\n",
        "\n",
        "    # 打开静态页面文件\n",
        "    with open('33.html', 'r', encoding='utf-8') as file:\n",
        "        content = file.read()\n",
        "\n",
        "        # 解析静态页面内容\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # 定位并提取商户数据\n",
        "        restaurant_list = soup.find_all('div', class_='poi-list-cell-desktop-right')\n",
        "\n",
        "        for restaurant in restaurant_list:\n",
        "            name = restaurant.find('div', class_='text').text.strip()\n",
        "\n",
        "            # 获取地址信息\n",
        "            address_info = restaurant.find('div', class_='poi-list-cell-line-info').text.strip()\n",
        "            address_parts = address_info.split('/')\n",
        "            address = address_parts[0].split('\\n')[0].strip()  # 取第一个斜杠之前的部分，并删除额外的空格和换行符\n",
        "\n",
        "            # 获取食物类型信息\n",
        "            food_type_info = restaurant.find('div', class_='poi-list-cell-line-info-details').text.strip()\n",
        "            food_types = [t.strip() for t in food_type_info.split(' / ')]\n",
        "\n",
        "            # 补充缺失的食物类型，最多只取前3个\n",
        "            while len(food_types) < 3:\n",
        "                food_types.append('')\n",
        "\n",
        "            region = food_types.pop(0)  # 第一个是 region，剩下的是食物类型\n",
        "            type1 = food_types.pop(0)  # 取出第一个食物类型作为 type1\n",
        "\n",
        "            # 判断是否有 type2 和 type3\n",
        "            if len(food_types) > 1:\n",
        "                type2 = food_types.pop(0)  # 取出第二个食物类型作为 type2\n",
        "                if '/' in type2:\n",
        "                    type2, type3 = type2.split('/', 1)\n",
        "                else:\n",
        "                    type3 = ''\n",
        "            else:\n",
        "                type2 = ''\n",
        "                type3 = ''\n",
        "\n",
        "            person_price = food_types.pop() if food_types else ''  # 最后一个是人均消费\n",
        "            well_rating = restaurant.find('div', class_='smile icon-wrapper big-score')\n",
        "            if well_rating:\n",
        "                well_rating = well_rating.text.strip()\n",
        "            else:\n",
        "                well_rating = ''  # 如果找不到评分信息，则将评分设置为空字符串\n",
        "\n",
        "            bad_rating = restaurant.find('div', class_='cry icon-wrapper')\n",
        "            if bad_rating:\n",
        "                bad_rating = bad_rating.text.strip()\n",
        "            else:\n",
        "                bad_rating = ''  # 如果找不到评分信息，则将评分设置为空字符串\n",
        "\n",
        "            # 将数据写入CSV文件\n",
        "            writer.writerow([name, address, region, type1, type2, type3, person_price, well_rating, bad_rating])\n",
        "\n",
        "print(\"数据获取并写入CSV文件完成。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIjBVruNzTZ2"
      },
      "source": [
        "##Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1_CmKjoiQlT"
      },
      "source": [
        "问题：\n",
        "1.网站阻止自动化工具: 网站采取措施阻止自动化工具（Selenium）访问其内容。可能是因为网站的安全策略或反爬虫措施。已尝试模拟真实用户行为，例如更改User-Agent头部信息或使用代理服务器，但是还是无法访问。\n",
        "2.静态网页获取的方式，只能获取250条数据，这跟我们自己在访问网站时候的情况一样，只能看到250家餐厅的数据。\n",
        "3.openrice没有开放API，无法通过API方式去访问。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# web scraping to openrice.com with scrapy"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kyOnxIXq8HE5",
        "6ajnOR3I7jBF",
        "jGyTm4nr8BFk",
        "_-tD6uz8W-3f"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
